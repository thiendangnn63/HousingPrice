{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ca38479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6d0255bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b9760c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']\n",
    "test_df['HouseAge'] = test_df['YrSold'] - test_df['YearBuilt']\n",
    "train_df['Total_SF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\n",
    "test_df['Total_SF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']\n",
    "train_df['Remodel_Age'] = train_df['YrSold'] - train_df['YearRemodAdd']\n",
    "test_df['Remodel_Age'] = test_df['YrSold'] - test_df['YearRemodAdd']\n",
    "\n",
    "train_df.drop(columns=['Id', 'YrSold', 'YearBuilt', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'YearRemodAdd'], inplace=True)\n",
    "test_df.drop(columns=['Id', 'YrSold', 'YearBuilt',  'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'YearRemodAdd'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "924e4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nan_thresh=0.5):\n",
    "        self.nan_thresh = nan_thresh\n",
    "        self.drop_columns_ = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        nan_ratio = X.isnull().sum() / len(X)\n",
    "        self.drop_columns_ = nan_ratio[nan_ratio > self.nan_thresh].index.tolist()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        if self.drop_columns_:\n",
    "            X_copy.drop(columns=self.drop_columns_, inplace=True)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "79e5a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, z_thresh=3.0):\n",
    "        self.z_thresh = z_thresh\n",
    "        self.bounds_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        numeric_cols = X.select_dtypes(include=np.number).columns\n",
    "        for col in numeric_cols:\n",
    "            mean = X[col].mean()\n",
    "            std = X[col].std()\n",
    "            upper_bound = mean + self.z_thresh * std\n",
    "            lower_bound = mean - self.z_thresh * std\n",
    "            self.bounds_[col] = (lower_bound, upper_bound)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col, (lower, upper) in self.bounds_.items():\n",
    "            if col in X_copy.columns:\n",
    "                X_copy[col] = X_copy[col].clip(lower=lower, upper=upper)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "82b578d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalTypeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        self.object_columns = []\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        print(\"\\n--- Fitting CategoricalTypeConverter ---\")\n",
    "        if not isinstance(x, pd.DataFrame):\n",
    "            x = pd.DataFrame(x)\n",
    "        \n",
    "        self.object_columns = x.select_dtypes(include='object').columns.tolist()\n",
    "        if not self.object_columns:\n",
    "            return self\n",
    "        \n",
    "        print(f\"Found object columns to encode: {self.object_columns}\")\n",
    "        self._encoder.fit(x[self.object_columns])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        print(\"\\n--- Transforming with CategoricalTypeConverter ---\")\n",
    "        if not isinstance(x, pd.DataFrame):\n",
    "            x = pd.DataFrame(x)\n",
    "        \n",
    "        xcopy = x.copy()\n",
    "\n",
    "        if not self.object_columns:\n",
    "            return xcopy\n",
    "        \n",
    "        encoded = self._encoder.transform(xcopy[self.object_columns])\n",
    "\n",
    "        encoded_df = pd.DataFrame(\n",
    "            encoded,\n",
    "            columns=self._encoder.get_feature_names_out(self.object_columns),\n",
    "            index=xcopy.index\n",
    "        )\n",
    "\n",
    "        xcopy.drop(columns=self.object_columns, inplace=True)\n",
    "        x_final = pd.concat([xcopy, encoded_df], axis=1)\n",
    "        \n",
    "        print(f\"Successfully encoded and added {len(encoded_df.columns)} new columns.\")\n",
    "        return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "54b6bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaNImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A robust imputer that learns all columns of a given type from the training\n",
    "    data and is prepared to impute any of them in the test data.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.numerical_imputer = None\n",
    "        self.categorical_imputer = None\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        if not isinstance(x, pd.DataFrame):\n",
    "            x = pd.DataFrame(x)\n",
    "        \n",
    "        self.numerical_cols = x.select_dtypes(include=np.number).columns.tolist()\n",
    "        self.categorical_cols = x.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "        if self.numerical_cols:\n",
    "            self.numerical_imputer = KNNImputer(n_neighbors=self.n_neighbors)\n",
    "            self.numerical_imputer.fit(x[self.numerical_cols])\n",
    "            \n",
    "        if self.categorical_cols:\n",
    "            self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            self.categorical_imputer.fit(x[self.categorical_cols])\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        xcopy = x.copy()\n",
    "        \n",
    "        # Apply the transformation to all learned columns.\n",
    "        if self.numerical_cols and self.numerical_imputer:\n",
    "            xcopy[self.numerical_cols] = self.numerical_imputer.transform(xcopy[self.numerical_cols])\n",
    "        \n",
    "        if self.categorical_cols and self.categorical_imputer:\n",
    "            xcopy[self.categorical_cols] = self.categorical_imputer.transform(xcopy[self.categorical_cols])\n",
    "        \n",
    "        return xcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "91fa356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('nan_dropper', ColumnDropper(nan_thresh=0.5)), \n",
    "    ('outlier_capper', OutlierCapper(z_thresh=3.0)),\n",
    "    ('imputer', NaNImputer(n_neighbors=5)),\n",
    "    ('encoder', CategoricalTypeConverter())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "15f24c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(xtrain, ytrain, xtest, ytest, model):\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'subsample': [0.6, 0.7, 0.8],\n",
    "        'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "        'n_estimators': [500, 1000, 2000],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=25,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(xtrain, ytrain)\n",
    "    print(\"\\n--- Hyperparameter Tuning Results ---\")\n",
    "    print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    ypred_log = best_model.predict(xtest)\n",
    "\n",
    "    ypred_original = np.expm1(ypred_log)\n",
    "    ytest_original = np.expm1(ytest)\n",
    "    \n",
    "    mse = mean_squared_error(ytest_original, ypred_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
    "    print(f'Root Mean Squared Error (RMSE): ${rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7cd2a63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fitting CategoricalTypeConverter ---\n",
      "Found object columns to encode: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
      "\n",
      "--- Transforming with CategoricalTypeConverter ---\n",
      "Successfully encoded and added 235 new columns.\n"
     ]
    }
   ],
   "source": [
    "model1 = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "x = train_df.drop(columns=['SalePrice'])\n",
    "x = preprocessing_pipeline.fit_transform(x)\n",
    "y = np.log1p(train_df['SalePrice'])\n",
    "\n",
    "# xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "# train_and_test(xtrain, ytrain, xtest, ytest, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f31a9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = xgb.XGBRegressor(\n",
    "#     subsample=0.6,\n",
    "#     n_estimators=1000,\n",
    "#     max_depth=6,\n",
    "#     learning_rate=0.01,\n",
    "#     gamma=0,\n",
    "#     colsample_bytree=0.6\n",
    "# )\n",
    "# model2.fit(x, y)\n",
    "# test_pred = np.expm1(model2.predict(preprocessing_pipeline.transform(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7728ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Lasso': Pipeline([('scaler', StandardScaler()), ('model', Lasso(alpha=0.0005, random_state=42))]),\n",
    "    'Ridge': Pipeline([('scaler', StandardScaler()), ('model', Ridge(alpha=10.0, random_state=42))]),\n",
    "    'ElasticNet': Pipeline([('scaler', StandardScaler()), ('model', ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=42))]),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=500, max_depth=6, random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingRegressor(n_estimators=1000, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=4, random_state=42, n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "88fa1bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Transforming with CategoricalTypeConverter ---\n",
      "Successfully encoded and added 235 new columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_df = preprocessing_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9d5fab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageYrBlt, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, MoSold, HouseAge, Total_SF, Remodel_Age, MSZoning_C (all), MSZoning_FV, MSZoning_RH, MSZoning_RL, MSZoning_RM, Street_Grvl, Street_Pave, LotShape_IR1, LotShape_IR2, LotShape_IR3, LotShape_Reg, LandContour_Bnk, LandContour_HLS, LandContour_Low, LandContour_Lvl, Utilities_AllPub, Utilities_NoSeWa, LotConfig_Corner, LotConfig_CulDSac, LotConfig_FR2, LotConfig_FR3, LotConfig_Inside, LandSlope_Gtl, LandSlope_Mod, LandSlope_Sev, Neighborhood_Blmngtn, Neighborhood_Blueste, Neighborhood_BrDale, Neighborhood_BrkSide, Neighborhood_ClearCr, Neighborhood_CollgCr, Neighborhood_Crawfor, Neighborhood_Edwards, Neighborhood_Gilbert, Neighborhood_IDOTRR, Neighborhood_MeadowV, Neighborhood_Mitchel, Neighborhood_NAmes, Neighborhood_NPkVill, Neighborhood_NWAmes, Neighborhood_NoRidge, Neighborhood_NridgHt, Neighborhood_OldTown, Neighborhood_SWISU, Neighborhood_Sawyer, Neighborhood_SawyerW, Neighborhood_Somerst, Neighborhood_StoneBr, Neighborhood_Timber, Neighborhood_Veenker, Condition1_Artery, Condition1_Feedr, Condition1_Norm, Condition1_PosA, Condition1_PosN, Condition1_RRAe, Condition1_RRAn, Condition1_RRNe, Condition1_RRNn, Condition2_Artery, Condition2_Feedr, Condition2_Norm, Condition2_PosA, Condition2_PosN, Condition2_RRAe, Condition2_RRAn, Condition2_RRNn, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 268 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5de7df82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training models and making predictions ---\n",
      "Training Lasso...\n",
      "Training Ridge...\n",
      "Training ElasticNet...\n",
      "Training RandomForest...\n",
      "Training GradientBoosting...\n",
      "Training XGBoost...\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "\n",
    "print(\"--- Training models and making predictions ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(x, y)\n",
    "    predictions[name] = model.predict(test_df)\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "ensemble_predictions_log = predictions_df.mean(axis=1)\n",
    "final_predictions = np.expm1(ensemble_predictions_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "578a4ff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[150]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m submission_df = pd.DataFrame({\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mId\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mtest_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mId\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mSalePrice\u001b[39m\u001b[33m'\u001b[39m: final_predictions\n\u001b[32m      4\u001b[39m })\n\u001b[32m      6\u001b[39m submission_df.to_csv(\u001b[33m'\u001b[39m\u001b[33msubmission.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSubmission file \u001b[39m\u001b[33m'\u001b[39m\u001b[33msubmission.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Id'"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_df['Id'],\n",
    "    'SalePrice': final_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully!\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ee3fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
