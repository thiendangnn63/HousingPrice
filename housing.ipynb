{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca38479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics  import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d0255bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_id = test_df['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9760c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['HouseAge'] = train_df['YrSold'] - train_df['YearBuilt']\n",
    "test_df['HouseAge'] = test_df['YrSold'] - test_df['YearBuilt']\n",
    "train_df['Total_SF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\n",
    "test_df['Total_SF'] = test_df['TotalBsmtSF'] + test_df['1stFlrSF'] + test_df['2ndFlrSF']\n",
    "train_df['Remodel_Age'] = train_df['YrSold'] - train_df['YearRemodAdd']\n",
    "test_df['Remodel_Age'] = test_df['YrSold'] - test_df['YearRemodAdd']\n",
    "\n",
    "train_df.drop(columns=['Id', 'YrSold', 'YearBuilt', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'YearRemodAdd'], inplace=True)\n",
    "test_df.drop(columns=['Id', 'YrSold', 'YearBuilt',  'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'YearRemodAdd'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "924e4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nan_thresh=0.5):\n",
    "        self.nan_thresh = nan_thresh\n",
    "        self.drop_columns_ = []\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        nan_ratio = X.isnull().sum() / len(X)\n",
    "        self.drop_columns_ = nan_ratio[nan_ratio > self.nan_thresh].index.tolist()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        if self.drop_columns_:\n",
    "            X_copy.drop(columns=self.drop_columns_, inplace=True)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79e5a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierCapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, z_thresh=3.0):\n",
    "        self.z_thresh = z_thresh\n",
    "        self.bounds_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        numeric_cols = X.select_dtypes(include=np.number).columns\n",
    "        for col in numeric_cols:\n",
    "            mean = X[col].mean()\n",
    "            std = X[col].std()\n",
    "            upper_bound = mean + self.z_thresh * std\n",
    "            lower_bound = mean - self.z_thresh * std\n",
    "            self.bounds_[col] = (lower_bound, upper_bound)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for col, (lower, upper) in self.bounds_.items():\n",
    "            if col in X_copy.columns:\n",
    "                X_copy[col] = X_copy[col].clip(lower=lower, upper=upper)\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82b578d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalTypeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        self.object_columns = []\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        print(\"\\n--- Fitting CategoricalTypeConverter ---\")\n",
    "        if not isinstance(x, pd.DataFrame):\n",
    "            x = pd.DataFrame(x)\n",
    "        \n",
    "        self.object_columns = x.select_dtypes(include='object').columns.tolist()\n",
    "        if not self.object_columns:\n",
    "            return self\n",
    "        \n",
    "        print(f\"Found object columns to encode: {self.object_columns}\")\n",
    "        self._encoder.fit(x[self.object_columns])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        print(\"\\n--- Transforming with CategoricalTypeConverter ---\")\n",
    "        if not isinstance(x, pd.DataFrame):\n",
    "            x = pd.DataFrame(x)\n",
    "        \n",
    "        xcopy = x.copy()\n",
    "\n",
    "        if not self.object_columns:\n",
    "            return xcopy\n",
    "        \n",
    "        encoded = self._encoder.transform(xcopy[self.object_columns])\n",
    "\n",
    "        encoded_df = pd.DataFrame(\n",
    "            encoded,\n",
    "            columns=self._encoder.get_feature_names_out(self.object_columns),\n",
    "            index=xcopy.index\n",
    "        )\n",
    "\n",
    "        xcopy.drop(columns=self.object_columns, inplace=True)\n",
    "        x_final = pd.concat([xcopy, encoded_df], axis=1)\n",
    "        \n",
    "        print(f\"Successfully encoded and added {len(encoded_df.columns)} new columns.\")\n",
    "        return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54b6bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaNImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.numerical_imputer = None\n",
    "        self.categorical_imputer = None\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        if not isinstance(x, pd.DataFrame):\n",
    "            x = pd.DataFrame(x)\n",
    "        \n",
    "        self.numerical_cols = x.select_dtypes(include=np.number).columns.tolist()\n",
    "        self.categorical_cols = x.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "        if self.numerical_cols:\n",
    "            self.numerical_imputer = KNNImputer(n_neighbors=self.n_neighbors)\n",
    "            self.numerical_imputer.fit(x[self.numerical_cols])\n",
    "            \n",
    "        if self.categorical_cols:\n",
    "            self.categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            self.categorical_imputer.fit(x[self.categorical_cols])\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        xcopy = x.copy()\n",
    "        if self.numerical_cols and self.numerical_imputer:\n",
    "            xcopy[self.numerical_cols] = self.numerical_imputer.transform(xcopy[self.numerical_cols])\n",
    "        \n",
    "        if self.categorical_cols and self.categorical_imputer:\n",
    "            xcopy[self.categorical_cols] = self.categorical_imputer.transform(xcopy[self.categorical_cols])\n",
    "        \n",
    "        return xcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91fa356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = Pipeline(steps=[\n",
    "    ('nan_dropper', ColumnDropper(nan_thresh=0.5)), \n",
    "    ('outlier_capper', OutlierCapper(z_thresh=3.0)),\n",
    "    ('imputer', NaNImputer(n_neighbors=5)),\n",
    "    ('encoder', CategoricalTypeConverter())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbc4b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lasso = {\n",
    "    'model__alpha': [0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'model__alpha': [0.01, 0.1, 1, 10, 100, 200]\n",
    "}\n",
    "\n",
    "param_grid_elasticnet = {\n",
    "    'model__alpha': [0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10],\n",
    "    'model__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'max_features': ['sqrt', 'log2', 1.0],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [500, 1000, 2000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8],\n",
    "    'n_estimators': [500, 1000, 2000],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "param_list = [param_grid_lasso, param_grid_ridge, param_grid_elasticnet, param_grid_rf, param_grid_gb, param_grid_xgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15f24c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(xtrain, xtest, ytrain, ytest, model, idx):\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_list[idx],\n",
    "        n_iter=25,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    random_search.fit(xtrain, ytrain)\n",
    "    print(\"\\n--- Hyperparameter Tuning Results ---\")\n",
    "    print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    ypred_log = best_model.predict(xtest)\n",
    "\n",
    "    ypred_original = np.expm1(ypred_log)\n",
    "    ytest_original = np.expm1(ytest)\n",
    "    \n",
    "    mse = mean_squared_error(ytest_original, ypred_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
    "    print(f'Root Mean Squared Error (RMSE): ${rmse:.2f}')\n",
    "\n",
    "    return random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cd2a63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fitting CategoricalTypeConverter ---\n",
      "Found object columns to encode: ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
      "\n",
      "--- Transforming with CategoricalTypeConverter ---\n",
      "Successfully encoded and added 235 new columns.\n"
     ]
    }
   ],
   "source": [
    "model1 = xgb.XGBRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "x = train_df.drop(columns=['SalePrice'])\n",
    "x = preprocessing_pipeline.fit_transform(x)\n",
    "y = np.log1p(train_df['SalePrice'])\n",
    "\n",
    "# xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "# train_and_test(xtrain, ytrain, xtest, ytest, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f31a9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = xgb.XGBRegressor(\n",
    "#     subsample=0.6,\n",
    "#     n_estimators=1000,\n",
    "#     max_depth=6,\n",
    "#     learning_rate=0.01,\n",
    "#     gamma=0,\n",
    "#     colsample_bytree=0.6\n",
    "# )\n",
    "# model2.fit(x, y)\n",
    "# test_pred = np.expm1(model2.predict(preprocessing_pipeline.transform(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7728ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Lasso': Pipeline([('scaler', StandardScaler()), ('model', Lasso(random_state=42))]),\n",
    "    'Ridge': Pipeline([('scaler', StandardScaler()), ('model', Ridge(random_state=42))]),\n",
    "    'ElasticNet': Pipeline([('scaler', StandardScaler()), ('model', ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=42))]),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=500, max_depth=6, random_state=42, n_jobs=-1),\n",
    "    'GradientBoosting': GradientBoostingRegressor(n_estimators=1000, learning_rate=0.05, max_depth=4, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=4, random_state=42, n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88fa1bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Transforming with CategoricalTypeConverter ---\n",
      "Successfully encoded and added 235 new columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_df = preprocessing_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d5fab77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageYrBlt, GarageCars, GarageArea, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, MoSold, HouseAge, Total_SF, Remodel_Age, MSZoning_C (all), MSZoning_FV, MSZoning_RH, MSZoning_RL, MSZoning_RM, Street_Grvl, Street_Pave, LotShape_IR1, LotShape_IR2, LotShape_IR3, LotShape_Reg, LandContour_Bnk, LandContour_HLS, LandContour_Low, LandContour_Lvl, Utilities_AllPub, Utilities_NoSeWa, LotConfig_Corner, LotConfig_CulDSac, LotConfig_FR2, LotConfig_FR3, LotConfig_Inside, LandSlope_Gtl, LandSlope_Mod, LandSlope_Sev, Neighborhood_Blmngtn, Neighborhood_Blueste, Neighborhood_BrDale, Neighborhood_BrkSide, Neighborhood_ClearCr, Neighborhood_CollgCr, Neighborhood_Crawfor, Neighborhood_Edwards, Neighborhood_Gilbert, Neighborhood_IDOTRR, Neighborhood_MeadowV, Neighborhood_Mitchel, Neighborhood_NAmes, Neighborhood_NPkVill, Neighborhood_NWAmes, Neighborhood_NoRidge, Neighborhood_NridgHt, Neighborhood_OldTown, Neighborhood_SWISU, Neighborhood_Sawyer, Neighborhood_SawyerW, Neighborhood_Somerst, Neighborhood_StoneBr, Neighborhood_Timber, Neighborhood_Veenker, Condition1_Artery, Condition1_Feedr, Condition1_Norm, Condition1_PosA, Condition1_PosN, Condition1_RRAe, Condition1_RRAn, Condition1_RRNe, Condition1_RRNn, Condition2_Artery, Condition2_Feedr, Condition2_Norm, Condition2_PosA, Condition2_PosN, Condition2_RRAe, Condition2_RRAn, Condition2_RRNn, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 268 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5de7df82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training models and making predictions ---\n",
      "Training Lasso...\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 7 is smaller than n_iter=25. Running 7 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.810e-02, tolerance: 1.456e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter Tuning Results ---\n",
      "Best parameters found: {'model__alpha': 0.01}\n",
      "Mean Squared Error (MSE): 356885593.92\n",
      "Root Mean Squared Error (RMSE): $18891.42\n",
      "Training Ridge...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 6 is smaller than n_iter=25. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter Tuning Results ---\n",
      "Best parameters found: {'model__alpha': 200}\n",
      "Mean Squared Error (MSE): 330597599.43\n",
      "Root Mean Squared Error (RMSE): $18182.34\n",
      "Training ElasticNet...\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.486e-01, tolerance: 1.513e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.074e-01, tolerance: 1.456e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.747e+00, tolerance: 1.456e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.833e-02, tolerance: 1.385e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e-01, tolerance: 1.388e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.420e-02, tolerance: 1.456e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.037e-01, tolerance: 1.456e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.137e-02, tolerance: 1.456e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.527e-02, tolerance: 1.385e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.925e-02, tolerance: 1.456e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.114e-02, tolerance: 1.456e-02\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter Tuning Results ---\n",
      "Best parameters found: {'model__l1_ratio': 0.5, 'model__alpha': 0.01}\n",
      "Mean Squared Error (MSE): 292584509.24\n",
      "Root Mean Squared Error (RMSE): $17105.10\n",
      "Training RandomForest...\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "\n",
      "--- Hyperparameter Tuning Results ---\n",
      "Best parameters found: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 1.0, 'max_depth': 20}\n",
      "Mean Squared Error (MSE): 588792954.77\n",
      "Root Mean Squared Error (RMSE): $24265.06\n",
      "Training GradientBoosting...\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "\n",
      "--- Hyperparameter Tuning Results ---\n",
      "Best parameters found: {'subsample': 0.6, 'n_estimators': 2000, 'max_features': 'sqrt', 'max_depth': 5, 'learning_rate': 0.01}\n",
      "Mean Squared Error (MSE): 380576159.12\n",
      "Root Mean Squared Error (RMSE): $19508.36\n",
      "Training XGBoost...\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "\n",
      "--- Hyperparameter Tuning Results ---\n",
      "Best parameters found: {'subsample': 0.6, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.01, 'gamma': 0, 'colsample_bytree': 0.6}\n",
      "Mean Squared Error (MSE): 371478263.38\n",
      "Root Mean Squared Error (RMSE): $19273.77\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "best_params = {}\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)\n",
    "idx = 0\n",
    "\n",
    "print(\"--- Training models and making predictions ---\")\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    best_params[name] = train_and_test(xtrain, xtest, ytrain, ytest, model, idx)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4f75825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized all tuned models.\n"
     ]
    }
   ],
   "source": [
    "best_params_cleaned = {}\n",
    "for name, params in best_params.items():\n",
    "    if name in ['Lasso', 'Ridge', 'ElasticNet']:\n",
    "        cleaned_params = {key.replace('model__', ''): value for key, value in params.items()}\n",
    "        best_params_cleaned[name] = cleaned_params\n",
    "    else:\n",
    "        best_params_cleaned[name] = params\n",
    "        \n",
    "tuned_models = {\n",
    "    'XGBoost': xgb.XGBRegressor(\n",
    "        **best_params_cleaned['XGBoost'],\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingRegressor(\n",
    "        **best_params_cleaned['GradientBoosting'],\n",
    "        random_state=42\n",
    "    ),\n",
    "    'RandomForest': RandomForestRegressor(\n",
    "        **best_params_cleaned['RandomForest'],\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Lasso': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Lasso(**best_params_cleaned['Lasso'], random_state=42))\n",
    "    ]),\n",
    "    'Ridge': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', Ridge(**best_params_cleaned['Ridge'], random_state=42))\n",
    "    ]),\n",
    "    'ElasticNet': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', ElasticNet(**best_params_cleaned['ElasticNet'], random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Successfully initialized all tuned models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33baacc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training final ensemble models on full training data ---\n",
      "Training XGBoost...\n",
      "Training GradientBoosting...\n",
      "Training RandomForest...\n",
      "Training Lasso...\n",
      "Training Ridge...\n",
      "Training ElasticNet...\n",
      "\n",
      "--- Final ensemble predictions generated successfully! ---\n"
     ]
    }
   ],
   "source": [
    "final_model_predictions = {}\n",
    "\n",
    "print(\"\\n--- Training final ensemble models on full training data ---\")\n",
    "for name, model in tuned_models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(x, y)\n",
    "    final_model_predictions[name] = model.predict(test_df)\n",
    "\n",
    "predictions_df = pd.DataFrame(final_model_predictions)\n",
    "\n",
    "ensemble_predictions_log = predictions_df.mean(axis=1)\n",
    "\n",
    "final_predictions = np.expm1(ensemble_predictions_log)\n",
    "\n",
    "print(\"\\n--- Final ensemble predictions generated successfully! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "546c4ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = list(tuned_models.items())\n",
    "stacking = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=RidgeCV(),\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6240bbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking.fit(x, y)\n",
    "final_predictions = np.expm1(stacking.predict(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "578a4ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission.csv' created successfully!\n",
      "     Id      SalePrice\n",
      "0  1461  123636.365258\n",
      "1  1462  161488.591214\n",
      "2  1463  186743.460412\n",
      "3  1464  196524.789047\n",
      "4  1465  188969.175155\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'Id': test_id,\n",
    "    'SalePrice': final_predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully!\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060f14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
